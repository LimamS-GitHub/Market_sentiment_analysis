## ü§ñ Comprendre les Transformers

Les mod√®les utilis√©s dans notre analyse sont bas√©s sur l‚Äôarchitecture Transformer, une technologie r√©volutionnaire introduite en 2017 (Vaswani et al.).  
Contrairement aux RNNs ou LSTMs, ils traitent l‚Äôensemble d‚Äôun texte en parall√®le gr√¢ce √† un m√©canisme d‚Äô**attention**.

üëâ Pour une visualisation interactive de leur fonctionnement, nous vous recommandons cette ressource :

üîó [Transformer Visualizer (Polo Club)](https://poloclub.github.io/transformer-explainer/)

Dans notre projet, nous utilisons des versions sp√©cialis√©es du Transformer, pr√©entra√Æn√©es sur des textes financiers (ex. : FinancialBERT, DeBERTa-v3-fin) pour analyser les tweets li√©s √† Tesla.

# Explication de l'utilisation des mod√®les Transformers

Dans notre projet, les mod√®les Transformers ont √©t√© utilis√©s pour analyser le sentiment des tweets collect√©s au sujet de Tesla (TSLA).  
Chaque tweet a √©t√© pass√© dans un ou plusieurs mod√®les pr√©-entra√Æn√©s afin d'√©valuer s‚Äôil exprimait une opinion positive, neutre ou n√©gative.

---
![Analyse de sentiment via Transformers](diagramme_transformers1.png)




---

## Ressource pour mieux comprendre les Transformers

Pour mieux comprendre le fonctionnement des mod√®les Transformers et le principe de l'attention, nous recommandons la ressource suivante, particuli√®rement claire et interactive :

[Transformer Visualizer ‚Äì Polo Club](https://poloclub.github.io/transformer-explainer/)

Ce site illustre de mani√®re visuelle la fa√ßon dont les mots d‚Äôune phrase sont analys√©s et mis en relation les uns avec les autres dans un mod√®le Transformer.

---
# Analyse des mod√®les de sentiment

Dans cette partie, nous revenons en d√©tail sur les **mod√®les de sentiment utilis√©s dans notre projet**, en nous concentrant sur leur fonctionnement th√©orique.  
Une pr√©sentation g√©n√©rale des mod√®les (VADER, Transformers financiers, etc.) est d√©j√† propos√©e dans la section pr√©c√©dente :  
‚û°Ô∏è Voir : [Collecte & Pr√©paration des Tweets](tweet_collection.md)

Ici, nous approfondissons deux aspects essentiels :

1. Le fonctionnement lexical et math√©matique de **VADER**  
2. Le principe g√©n√©ral des **Transformers**, en lien avec leur usage pour l‚Äôanalyse de sentiment

---

## 1. VADER : un mod√®le lexical bas√© sur des r√®gles

**VADER** (Valence Aware Dictionary and sEntiment Reasoner) est un outil con√ßu pour d√©tecter l‚Äôopinion exprim√©e dans des textes courts comme les tweets.  
Il ne repose pas sur l‚Äôapprentissage automatique, mais sur un **dictionnaire de mots pr√©-scor√©s** et un ensemble de **r√®gles linguistiques**.

### Principes de base

Chaque mot du texte est compar√© √† un lexique contenant des scores allant de ‚àí4 √† +4, selon leur charge √©motionnelle.  
Des r√®gles modifient ce score selon :

- les majuscules (`GOOD` est plus fort que `good`) ;
- la ponctuation (`!!` augmente l‚Äôintensit√©) ;
- les mots modificateurs (`very`, `kind of`, etc.) ;
- les n√©gations (`not good` devient n√©gatif).

### Formule utilis√©e

Le score final compos√© (entre ‚àí1 et +1) est calcul√© selon une formule de normalisation :

![Formule compound de VADER](formule_vader.png)
Le r√©sultat donne un score continu, facile √† interpr√©ter.

---

## 2. Les Transformers : mod√®les √† attention

Les mod√®les Transformers sont des r√©seaux de neurones profonds introduits par Vaswani et al. (2017).  
Ils sont devenus la base des syst√®mes modernes de traitement automatique du langage (NLP).

Contrairement aux approches lexicale ou s√©quentielle (comme RNN), les Transformers traitent tout le texte **en parall√®le**, en captant les relations entre les mots gr√¢ce au m√©canisme d‚Äô**attention**.

Dans notre projet, nous avons utilis√© des versions pr√©entra√Æn√©es et sp√©cialis√©es en finance :

- `ProsusAI/finbert`
- `deberta-v3-financial-news-sentiment`
- `distilroberta-financial-news-sentiment`

Ces mod√®les attribuent √† chaque tweet une pr√©diction (`POSITIVE`, `NEUTRAL`, `NEGATIVE`), que nous avons convertie en :  
**+1**, **0** ou **‚àí1** pour les traitements ult√©rieurs.

---

## 3. Comparaison VADER vs Transformers

| Crit√®re | VADER | Transformers financiers |
|--------|-------|--------------------------|
| Approche | Lexicale (bas√©e r√®gles) | Apprentissage profond |
| Donn√©es requises | Aucune | Mod√®les pr√©-entra√Æn√©s |
| Vitesse | Tr√®s rapide | Plus lente √† l‚Äôinf√©rence |
| Capacit√© contextuelle | Limit√©e | √âlev√©e |
| Adaptation au langage boursier | Faible | Excellente |
| Interpr√©tabilit√© | Tr√®s bonne | Moyenne |

Nous avons utilis√© VADER comme point de r√©f√©rence simple, et les Transformers pour une analyse plus fine et sp√©cialis√©e.

---

## 4. Pour aller plus loin

Pour mieux comprendre le fonctionnement des Transformers, nous recommandons cette ressource interactive :

[Visualisation interactive des Transformers (Polo Club)](https://poloclub.github.io/transformer-explainer/)

---

Dans la section suivante, nous explorerons les scores produits et leur √©volution dans le temps, en lien avec le march√© boursier.


---
Dans notre projet, VADER est utilis√© **en parall√®le** des mod√®les Transformers.  
Il fournit une **mesure de r√©f√©rence rapide**, que nous comparons aux scores plus complexes g√©n√©r√©s par FinBERT, DeBERTa, etc.

Dans la section suivante, nous utiliserons ces scores pour explorer les liens entre sentiment agr√©g√© et performance boursi√®re.
