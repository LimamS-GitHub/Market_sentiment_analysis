## ü§ñ Comprendre les Transformers

Les mod√®les utilis√©s dans notre analyse sont bas√©s sur l‚Äôarchitecture Transformer, une technologie r√©volutionnaire introduite en 2017 (Vaswani et al.).  
Contrairement aux RNNs ou LSTMs, ils traitent l‚Äôensemble d‚Äôun texte en parall√®le gr√¢ce √† un m√©canisme d‚Äô**attention**.

üëâ Pour une visualisation interactive de leur fonctionnement, nous vous recommandons cette ressource :

üîó [Transformer Visualizer (Polo Club)](https://poloclub.github.io/transformer-explainer/)

Dans notre projet, nous utilisons des versions sp√©cialis√©es du Transformer, pr√©entra√Æn√©es sur des textes financiers (ex. : FinancialBERT, DeBERTa-v3-fin) pour analyser les tweets li√©s √† Tesla.

# Explication de l'utilisation des mod√®les Transformers

Dans notre projet, les mod√®les Transformers ont √©t√© utilis√©s pour analyser le sentiment des tweets collect√©s au sujet de Tesla (TSLA).  
Chaque tweet a √©t√© pass√© dans un ou plusieurs mod√®les pr√©-entra√Æn√©s afin d'√©valuer s‚Äôil exprimait une opinion positive, neutre ou n√©gative.

---
![Analyse de sentiment via Transformers](diagramme_transformers1.png)




---

## Ressource pour mieux comprendre les Transformers

Pour mieux comprendre le fonctionnement des mod√®les Transformers et le principe de l'attention, nous recommandons la ressource suivante, particuli√®rement claire et interactive :

[Transformer Visualizer ‚Äì Polo Club](https://poloclub.github.io/transformer-explainer/)

Ce site illustre de mani√®re visuelle la fa√ßon dont les mots d‚Äôune phrase sont analys√©s et mis en relation les uns avec les autres dans un mod√®le Transformer.

---
# Analyse des mod√®les de sentiment

Dans cette partie, nous revenons en d√©tail sur les **mod√®les de sentiment utilis√©s dans notre projet**, en nous concentrant sur leur fonctionnement th√©orique.  
Une pr√©sentation g√©n√©rale des mod√®les (VADER, Transformers financiers, etc.) est d√©j√† propos√©e dans la section pr√©c√©dente :  
‚û°Ô∏è Voir : [Collecte & Pr√©paration des Tweets](tweet_collection.md)

Ici, nous approfondissons deux aspects essentiels :

1. Le fonctionnement lexical et math√©matique de **VADER**  
2. Le principe g√©n√©ral des **Transformers**, avec un sch√©ma de leur usage dans notre pipeline

---

## 1. VADER : un mod√®le lexical bas√© sur des r√®gles

**VADER** (Valence Aware Dictionary and sEntiment Reasoner) est un outil con√ßu pour d√©tecter l‚Äôopinion exprim√©e dans des textes courts comme les tweets.  
Il ne repose pas sur l‚Äôapprentissage automatique, mais sur un **dictionnaire de mots pr√©-scor√©s** et un ensemble de **r√®gles linguistiques**.

### Principes de base

Chaque mot du texte est compar√© √† un lexique contenant des scores allant de ‚àí4 √† +4, selon leur charge √©motionnelle.  
Des r√®gles modifient ce score selon :

- les majuscules (`GOOD` est plus fort que `good`) ;
- la ponctuation (`!!` augmente l‚Äôintensit√©) ;
- les mots modificateurs (`very`, `kind of`, etc.) ;
- les n√©gations (`not good` devient n√©gatif).

### Formule utilis√©e

Le score final compos√© (entre ‚àí1 et +1) est calcul√© selon une formule de normalisation :

\[
\text{compound} = \frac{\sum_{i=1}^{n} s_i}{\sqrt{\sum_{i=1}^{n} s_i^2 + \alpha}}
\]

o√π :

- \( s_i \) est le score (positif ou n√©gatif) de chaque √©l√©ment du tweet ;
- \( \alpha \) est un facteur de normalisation (valeur par d√©faut : 15).

Le r√©sultat donne un score continu, facile √† interpr√©ter.

---

## 2. Les Transformers : mod√®les √† attention

Les mod√®les Transformers sont des r√©seaux de neurones profonds introduits par Vaswani et al. (2017).  
Ils sont devenus la base des syst√®mes modernes de traitement automatique du langage (NLP).

Contrairement aux approches lexicale ou s√©quentielle (comme RNN), les Transformers traitent tout le texte **en parall√®le**, en captant les relations entre les mots gr√¢ce au m√©canisme d‚Äô**attention**.

Dans notre projet, nous avons utilis√© des versions pr√©entra√Æn√©es et sp√©cialis√©es en finance :

- `ProsusAI/finbert`
- `deberta-v3-financial-news-sentiment`
- `distilroberta-financial-news-sentiment`

Ces mod√®les attribuent √† chaque tweet une pr√©diction (`POSITIVE`, `NEUTRAL`, `NEGATIVE`), que nous avons convertie en :  
**+1**, **0** ou **‚àí1** pour les traitements ult√©rieurs.

---

## 3. Sch√©ma de traitement avec Transformers

Le sch√©ma ci-dessous illustre l'encha√Ænement des √©tapes appliqu√©es √† chaque tweet lors de l‚Äôanalyse de sentiment avec les mod√®les Transformers.

```mermaid
flowchart TD
  A[Etape 1 : Tweet brut] --> B[Nettoyage du texte]
  B --> C[D√©tection de la langue]
  C --> D{Langue = anglais ?}
  D -- Oui --> E[Envoi au mod√®le Transformer]
  E --> F[Pr√©diction : POS / NEU / NEG]
  F --> G[Conversion : +1 / 0 / -1]
  G --> H[Enregistrement dans CSV]
  D -- Non --> X[Rejet du tweet]

  subgraph Mod√®les utilis√©s
    M1[FinBERT]
    M2[DeBERTa-v3-fin]
    M3[DistilRoBERTa-fin]
  end

  E --> M1
  E --> M2
  E --> M3


---
Dans notre projet, VADER est utilis√© **en parall√®le** des mod√®les Transformers.  
Il fournit une **mesure de r√©f√©rence rapide**, que nous comparons aux scores plus complexes g√©n√©r√©s par FinBERT, DeBERTa, etc.

Dans la section suivante, nous utiliserons ces scores pour explorer les liens entre sentiment agr√©g√© et performance boursi√®re.
