# üß† Explication des mod√®les de sentiment utilis√©s

Dans ce chapitre, nous d√©taillons les mod√®les d‚Äôanalyse de sentiment utilis√©s pour enrichir les tweets relatifs √† Tesla.  
Nous pr√©sentons d‚Äôabord **VADER**, une approche lexicale bas√©e sur des r√®gles, puis les **mod√®les Transformers** pr√©-entra√Æn√©s adapt√©s au langage financier.

---

## 1. VADER : une approche lexicale bas√©e sur des r√®gles

**VADER** (Valence Aware Dictionary and sEntiment Reasoner) est un outil con√ßu pour l‚Äôanalyse de sentiment dans des textes courts tels que les tweets.  
Il repose sur un **lexique de mots scor√©s** et un ensemble de **r√®gles linguistiques** sans apprentissage automatique.

### Fonctionnement

Chaque mot est associ√© √† un score de valence compris entre ‚àí4 et +4.  
Ce score peut √™tre modul√© par :

- des majuscules (`GREAT` est plus fort que `great`) ;
- des ponctuations (`!!!`) ;
- des modificateurs d‚Äôintensit√© (`very`, `slightly`, etc.) ;
- des n√©gations (`not good`, `isn't bad`).

### Formule utilis√©e

Le score final (appel√© *compound*) est normalis√© dans l‚Äôintervalle [‚àí1, +1] √† l‚Äôaide de la formule :

$$\text{compound} = \frac{\sum s_i}{\sqrt{\sum s_i^2 + \alpha}}$$

O√π :
- s·µ¢ est le score de chaque mot ou modificateur
- Œ± est une constante (valeur par d√©faut : 15)

Le r√©sultat donne un score unique refl√©tant la tonalit√© globale du tweet.

---

## 2. comprendre Les Transformers : mod√®les contextuels par attention

Les **Transformers** sont des mod√®les de langage introduits par Vaswani et al. (2017), fond√©s sur le m√©canisme d‚Äô**attention**.  
Contrairement aux approches s√©quentielles (RNN, LSTM), ils traitent l‚Äôensemble du texte en parall√®le et captent les d√©pendances entre mots, m√™me distants.

### Fonctionnement g√©n√©ral

Chaque mot est converti en un vecteur, puis compar√© aux autres mots du texte via des **poids d‚Äôattention**.  
Cela permet de mod√©liser le contexte d‚Äôun mot selon sa relation avec les autres termes.

### Mod√®les utilis√©s dans notre projet

Nous avons appliqu√© plusieurs Transformers sp√©cialis√©s dans le domaine financier :

- `ProsusAI/finbert`
- `deberta-v3-financial-news-sentiment`
- `distilroberta-financial-news-sentiment`

Chaque tweet est analys√© individuellement, et le mod√®le retourne une **classe de sentiment** :

- `POSITIVE` ‚Üí **+1**  
- `NEUTRAL` ‚Üí **0**  
- `NEGATIVE` ‚Üí **‚àí1**

Ces scores sont ensuite int√©gr√©s dans notre base de donn√©es.

---
### üéØ Le principe de l‚Äôattention

Chaque mot dans une phrase va chercher √† comprendre **√† quels autres mots il doit faire attention** pour bien interpr√©ter le sens global.  
Cela repose sur trois vecteurs :

- **Q (Query)** : ce que le mot cherche √† comprendre
- **K (Key)** : les mots potentiellement utiles
- **V (Value)** : les informations que ces mots contiennent

Le poids d‚Äôattention entre deux mots est calcul√© par :

$$
\text{Attention}(Q_i, K_j) = \frac{Q_i \cdot K_j}{\sqrt{d_k}}
$$

o√π :

- \( Q_i \cdot K_j \) est le produit scalaire entre les vecteurs de requ√™te et de cl√©,
- \( d_k \) est la dimension des vecteurs cl√©s, utilis√©e comme facteur de normalisation.

Ensuite, une **fonction softmax** est appliqu√©e pour convertir les scores en **poids positifs** dont la somme est √©gale √† 1 (distribution de probabilit√©).

## 3. Comparaison des deux approches

| Crit√®re                        | VADER                     | Transformers financiers         |
|-------------------------------|---------------------------|---------------------------------|
| Approche                      | Lexicale (bas√©e sur r√®gles) | Apprentissage profond (NLP)    |
| Donn√©es requises              | Aucune                    | Corpus pr√©-entra√Æn√©s massifs    |
| Vitesse                       | Tr√®s rapide               | Plus lente                      |
| Capacit√© √† comprendre le contexte | Limit√©e                 | √âlev√©e                          |
| Adaptation au domaine financier| Faible                    | Excellente                      |
| Interpr√©tabilit√©              | Tr√®s bonne                | Moyenne √† faible                |

Nous avons utilis√© VADER comme **point de r√©f√©rence rapide** et facilement interpr√©table, tandis que les Transformers ont √©t√© mobilis√©s pour fournir une **analyse fine**, tenant compte du langage sp√©cifique √† la finance.

---


Dans la section suivante, nous analyserons comment les scores de sentiment obtenus √©voluent dans le temps et comment ils sont corr√©l√©s avec les cours boursiers de Tesla.
