# ğŸ¦ Tweet Scraper via Nitter + Sentiment Analysis

Ce projet permet de **scraper des tweets publics** depuis [Nitter](https://nitter.net), une alternative sans JavaScript Ã  Twitter, et d'appliquer plusieurs **modÃ¨les d'analyse de sentiment**. Il est conÃ§u pour contourner les limitations d'accÃ¨s via un systÃ¨me de **rotation de proxies HTTPS**.

## ğŸ¯ Objectif

- RÃ©cupÃ©rer quotidiennement des tweets liÃ©s Ã  une entreprise donnÃ©e (ex : `Tesla`).
- Appliquer 5 modÃ¨les de sentiment (VADER + 4 Transformers financiers).
- Sauvegarder les rÃ©sultats par mois et dans un fichier global, pour analyse de tendance ou backtest boursier.

---

## âš™ï¸ DÃ©pendances
```bash
pip install selenium webdriver-manager pandas langdetect beautifulsoup4 requests vaderSentiment transformers
```
Python 3.8+ recommandÃ©.

---

## ğŸš€ Lancer le scraping

```bash
python main.py
```

Cela lance le scraping pour la plage de dates dÃ©finie dans `main.py`, par dÃ©faut entre le 15 et 19 avril 2025.

---

## ğŸ”§ ParamÃ¨tres configurables (`main.py`)
- `start_date` / `end_date` : plage de dates Ã  scraper
- `company_name` : mot-clÃ© recherchÃ© dans les tweets (par dÃ©faut : "Tesla")
- `minimum_number_tweets_per_day` : nombre minimum de tweets Ã  collecter par jour

---

## ğŸ§© Fonctionnement (Ã©tapes principales)
1. **Initialisation des dates, modÃ¨les de sentiment et proxies**
2. **Pour chaque jour :**
   - Ouverture d'un navigateur avec proxy
   - Scraping sur Nitter
   - Nettoyage et filtrage des tweets en anglais
   - Analyse de sentiment avec VADER et 4 modÃ¨les Transformers
   - Enregistrement dans un buffer mensuel et global
3. **Ã€ chaque changement de mois :**
   - Ã‰criture dans un fichier `Data_for_YYYY-MM.csv`
4. **Ã€ la fin du script :**
   - Fusion et sauvegarde finale dans `Data_<company>.csv`

---

## ğŸ“Š SchÃ©ma du processus de scraping

```mermaid
graph TD
    Start[Start script] --> LoadParams[Load parameters]
    LoadParams --> LoopDates[Loop through dates]
    LoopDates --> GetProxy[Select valid proxy]
    GetProxy --> LaunchDriver[Init WebDriver with proxy]
    LaunchDriver --> AccessNitter[Access Nitter & search tweets]
    AccessNitter --> Extract[Extract & filter English tweets]
    Extract --> Clean[Clean tweet text]
    Clean --> Analyze[Sentiment analysis: VADER + models]
    Analyze --> MonthCheck{Month changed?}
    MonthCheck -- Yes --> SaveMonth[Save monthly CSV]
    MonthCheck -- No --> ContinueDate[Next date]
    SaveMonth --> ContinueDate
    ContinueDate --> FinalCheck{Last date?}
    FinalCheck -- No --> LoopDates
    FinalCheck -- Yes --> SaveAll[Save final CSV]
    SaveAll --> End[Done]
```

---

## ğŸ“ Structure des fichiers

- `main.py` : script principal de scraping et orchestration
- `scrape.py` : logique de navigation sur Nitter, extraction des tweets
- `driver.py` : initialisation du navigateur Chrome avec proxy
- `utils.py` : gÃ©nÃ©ration de dates, gestion des proxies, nettoyage texte
- `sentiment.py` : analyse de sentiment avec VADER + Transformers

---

## ğŸ“„ Format de sortie

Les tweets sont sauvegardÃ©s dans :
- des fichiers mensuels : `Data_for_2025-04.csv`
- un fichier global : `Data_Tesla.csv`

Colonnes principales :
| id         | query_date | text               | verified | CLEANED_TWEET | SENTIMENT_VADER | SENTIMENT_ModelName |
|------------|------------|--------------------|----------|----------------|------------------|----------------------|
| tweet_id   | yyyy-mm-dd | contenu du tweet   | True/False | tweet nettoyÃ©  | score [-1 Ã  1]   | score du modÃ¨le NLP  |

---

## ğŸ›¡ï¸ Gestion des erreurs & contournement
- **Proxies HTTPS** : RÃ©cupÃ©rÃ©s depuis `sslproxies.org`, testÃ©s automatiquement.
- **Rotation automatique** : En cas d'Ã©chec, on change de proxy.
- **Retry** : Jusqu'Ã  3 tentatives par jour si le scraping Ã©choue.
- **Filtrage linguistique** : Seuls les tweets dÃ©tectÃ©s comme anglais sont conservÃ©s (`langdetect`).

---

## ğŸ“¬ Contact
Pour toute amÃ©lioration ou suggestion, n'hÃ©site pas Ã  ouvrir une *issue* ou Ã  me contacter directement.

