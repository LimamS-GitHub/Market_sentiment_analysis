# üê¶ Tweet Scraper via Nitter + Sentiment Analysis

Ce projet permet de **scraper des tweets publics** depuis [Nitter](https://nitter.net), une alternative sans JavaScript √† Twitter, et d'appliquer plusieurs **mod√®les d'analyse de sentiment**. Il est con√ßu pour contourner les limitations d'acc√®s via un syst√®me de **rotation de proxies HTTPS**.

## Objectif

- R√©cup√©rer quotidiennement des tweets li√©s √† une entreprise donn√©e (ex : `Tesla`).
- Appliquer 4 mod√®les de sentiment (VADER + 3 Transformers financiers).
- Sauvegarder les r√©sultats par mois et dans un fichier global, pour analyse de tendance ou backtest boursier.

---

## D√©pendances

```bash
pip install selenium webdriver-manager pandas langdetect beautifulsoup4 requests vaderSentiment transformers
```

Python 3.8+ recommand√©.

---

## Lancer le scraping

```bash
python main.py
```

Cela lance le scraping pour la plage de dates d√©finie dans `main.py`, par d√©faut entre le 15 et 19 avril 2025.

---

## Param√®tres configurables (`main.py`)

- `start_date` / `end_date` : plage de dates √† scraper
- `company_name` : mot-cl√© recherch√© dans les tweets (par d√©faut : "Tesla")
- `minimum_number_tweets_per_day` : nombre minimum de tweets √† collecter par jour

---

## Fonctionnement (√©tapes principales)

1. **Initialisation des dates, mod√®les de sentiment et proxies**
2. **Pour chaque jour :**
   - Ouverture d'un navigateur avec proxy
   - Scraping sur Nitter
   - Nettoyage et filtrage des tweets en anglais
   - Analyse de sentiment avec VADER et 3 mod√®les Transformers
   - Enregistrement dans un buffer mensuel et global
3. **√Ä chaque changement de mois :**
   - √âcriture dans un fichier `Data_for_YYYY-MM.csv`
4. **√Ä la fin du script :**
   - Fusion et sauvegarde finale dans `Data_<company>.csv`

---

## Sch√©ma du processus de scraping

```mermaid
graph TD
    Start[Start script] --> LoadParams[Load parameters]
    LoadParams --> LoopDates[Loop through dates]
    LoopDates --> GetProxy[Select random valid proxy]
    GetProxy --> LaunchDriver[Init WebDriver with proxy]
    LaunchDriver --> AccessNitter[Access Nitter & search tweets]
    AccessNitter --> Extract[Extract & filter English tweets]
    Extract --> Clean[Clean tweet text]
    Clean --> Analyze[Sentiment analysis: VADER + models]
    Analyze --> MonthCheck{Month changed?}
    MonthCheck -- Yes --> SaveMonth[Save monthly CSV]
    MonthCheck -- No --> ContinueDate[Next date]
    SaveMonth --> ContinueDate
    ContinueDate --> FinalCheck{Last date?}
    FinalCheck -- No --> Retry{Retry needed?}
    Retry -- Yes --> GetProxy
    Retry -- No --> LoopDates
    FinalCheck -- Yes --> SaveAll[Save final CSV]
    SaveAll --> End[Done]
```

---

## Structure des fichiers

- `main.py` : script principal de scraping et orchestration
- `scrape.py` : logique de navigation sur Nitter, extraction des tweets
- `driver.py` : initialisation du navigateur Chrome avec proxy
- `utils.py` : g√©n√©ration de dates, gestion des proxies, nettoyage texte
- `sentiment.py` : analyse de sentiment avec VADER + Transformers

---

## Format de sortie

Les tweets sont sauvegard√©s dans :

- des fichiers mensuels : `Data_for_2025-04.csv`
- un fichier global : `Data_Tesla.csv`

Colonnes principales :

| id        | query\_date | text             | verified   | CLEANED\_TWEET | SENTIMENT\_VADER | SENTIMENT\_ModelName |
| --------- | ----------- | ---------------- | ---------- | -------------- | ---------------- | -------------------- |
| tweet\_id | yyyy-mm-dd  | contenu du tweet | True/False | tweet nettoy√©  | score [-1 √† 1]   | score du mod√®le NLP  |

---

## Gestion des erreurs & contournement

- **Proxies HTTPS dynamiques** : R√©cup√©r√©s depuis `sslproxies.org` puis filtr√©s gr√¢ce √† `valid_proxies()` pour ne garder que ceux qui fonctionnent.
- **Test unitaire de validit√©** : Chaque proxy est test√© individuellement via une requ√™te HTTPS vers Nitter (`test_https_proxy`).
- **Rotation intelligente** : Pour chaque tentative, un proxy est s√©lectionn√© au hasard parmi la liste des valides. Une fois utilis√©, il est retir√© temporairement pour √©viter les blocages.
- \*\*Retries\*\*: Jusqu'√† **10 tentatives par jour**, avec changement de proxy apr√®s chaque tentative √©chou√©e.
- **Filtrage linguistique** : Seuls les tweets d√©tect√©s comme √©tant en anglais sont conserv√©s (`langdetect`).Seuls les tweets d√©tect√©s comme √©tant en anglais sont conserv√©s (`langdetect`).

---

## üì¨ Contact

Pour toute am√©lioration ou suggestion, n'h√©site pas √† ouvrir une *issue* ou √† me contacter directement.

