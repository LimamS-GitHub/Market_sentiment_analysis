# ğŸ¦ Tweet Scraper via Nitter

Ce script permet de **scraper quotidiennement des tweets publics** depuis [Nitter](https://nitter.net), une alternative sans JavaScript Ã  Twitter, en utilisant Selenium. Il sâ€™appuie sur des **proxies HTTPS** pour Ã©viter les blocages IP.

## Objectif

RÃ©cupÃ©rer des tweets liÃ©s Ã  un mot-clÃ© donnÃ© (par dÃ©faut : `tesla`) sur une plage de dates donnÃ©e, en contournant les limites d'accÃ¨s grÃ¢ce Ã  des proxies publics.

Les donnÃ©es collectÃ©es peuvent ensuite Ãªtre utilisÃ©es pour des analyses de sentiment, d'opinion ou de tendance dans le temps.

---

## DÃ©pendances

Voici les principales bibliothÃ¨ques utilisÃ©es :

```bash
pip install selenium webdriver-manager pandas langdetect beautifulsoup4 requests
```

Python 3.8+ recommandÃ©.

---

## ExÃ©cution

### Lancer le scraping

```bash
python main.py
```

Cela lance un scraping de tweets pour chaque jour entre le 2 janvier 2023 et le 2 janvier 2025 (modifiable dans `main.py`).

---

## ParamÃ¨tres configurables

Modifiables directement dans `main.py` :

- `start_date` / `end_date` : plage de dates Ã  scraper
- `keyword` : mot-clÃ© recherchÃ© (par dÃ©faut `"tesla"`)
- `max_tweets_per_day` : nombre de tweets max par jour (par dÃ©faut `30`)

---

## Format de sortie

Chaque jour donne lieu Ã  un fichier `.csv` :

```bash
tesla_tweets_2023-01-02.csv
```

Format des colonnes :

| id         | query_date | text               | verified |
|------------|------------|--------------------|----------|
| tweet_id   | date-1     | contenu du tweet   | True/False |

---

## Conseils & Limites

- **Erreurs 429 / Captchas** : Nitter peut bloquer les requÃªtes si trop nombreuses. C'est pour cela que le script utilise :
  - des pauses alÃ©atoires entre les jours (`time.sleep`)
  - des **proxies HTTPS** automatiquement rÃ©cupÃ©rÃ©s depuis `sslproxies.org`
- **Retry automatique** : jusquâ€™Ã  10 tentatives par jour si le scraping Ã©choue.
- **Langue filtrÃ©e** : seuls les tweets dÃ©tectÃ©s en anglais (`langdetect`) sont conservÃ©s.

---

## ğŸ§ª Exemple de proxy utilisÃ©

Les proxies sont automatiquement testÃ©s via :

```python
test_https_proxy(proxy)
```

Et initialisÃ©s dans Chrome headless via :

```python
initialize_driver(proxy)
```

---

## ğŸ“ Structure des fichiers

- `main.py` : script principal de scraping
- `scrape.py` : logique de collecte via Selenium
- `driver.py` : configuration du navigateur
- `utils.py` : outils proxy, nettoyage texte, etc.

---

## ğŸ“¬ Contact

Pour toute amÃ©lioration ou question, n'hÃ©sitez pas Ã  ouvrir une *issue* ou Ã  me contacter directement.
